{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fukushima Heatmap: Subsecting Data\n",
    "===============================\n",
    "In some cases, the amount of data available is too much to graph all at once. While the Fukushima set is small enough to fit comfortably in memory, we can still use it to showcase some techniques for handling much larger sets. In this case, we will still process the entire set of data, but do so in coordinate chunks; each chunk is averaged to create a heatmap. You can configure the number of chunks created; the more there are, the less data is held in memory at a time, but the more queries are done overall.\n",
    "\n",
    "\n",
    "Initial setup and finding all dates\n",
    "--------------------------------------\n",
    "\n",
    "We configure our graph (including the extent to which we'll divide our data), open a connection to our database of interest, and find what dates are available to us. We'll track data from each date separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sina.datastores.sql as sina_sql\n",
    "\n",
    "DATABASE = '/collab/usr/gapps/wf/examples/data/fukushima/fukushima.sqlite'\n",
    "\n",
    "# Number of chunks along a side; this number squared is the total number of chunks\n",
    "CHUNKS_PER_SIDE = 12\n",
    "\n",
    "# Identify the coordinates, label, and label orientation for the power\n",
    "# plant and selected cities as points of reference.\n",
    "CITIES = [  # (lon, lat), desc, horizontal alignment\n",
    "    [(141.0281, 37.4213), ' Daiichi Nuclear Power Plant', 'left'],\n",
    "    [(141.0125, 37.4492), 'Futaba ', 'right'],\n",
    "    [(141.0000, 37.4833), ' Namie', 'left'],\n",
    "    [(140.9836, 37.4044), ' Okuma', 'right'],\n",
    "    [(141.0088, 37.3454), ' Tomioka', 'left']]\n",
    "\n",
    "\n",
    "# Configure how cities are marked\n",
    "COLOR_CITIES = 'red'\n",
    "AREA_CITIES = 12\n",
    "\n",
    "# The coordinates our analysis will cover\n",
    "X_COORDS = (140.9, 141.3)\n",
    "Y_COORDS = (37.0, 37.83)\n",
    "\n",
    "# The city coordinates need to be normalized to our grid (whose size depends on CHUNKS_PER_SIDE)\n",
    "norm_x = [CHUNKS_PER_SIDE*((c[0][0]-X_COORDS[0])/(X_COORDS[1]-X_COORDS[0])) for c in CITIES]\n",
    "norm_y = [CHUNKS_PER_SIDE*(1 - (c[0][1]-Y_COORDS[0])/(Y_COORDS[1]-Y_COORDS[0])) for c in CITIES]\n",
    "\n",
    "# Create the data access object factory.\n",
    "factory = sina_sql.DAOFactory(DATABASE)\n",
    "record_handler = factory.createRecordDAO()\n",
    "relationship_handler = factory.createRelationshipDAO()\n",
    "\n",
    "# Get the ids of the experiments (which are their dates)\n",
    "all_experiments = record_handler.get_all_of_type(\"exp\")\n",
    "dates = [str(x.id) for x in all_experiments]\n",
    "\n",
    "print('Config loaded. Database has the following dates available: {}'.format(', '.join(dates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the Data: Filtering Logic\n",
    "========================\n",
    "We subdivide our coordinate range (37.3-37.8, 140.9-141.4) into chunks_per_side^2 regions and find the records whose coordinates are within each range. We separate these out based on which day each Record is associated with. We then find that Record's gcnorm (counts per sec) and average to get that chunk's average for the day, and also track the total number of records per chunk per day (so we know around how confident we are in that average). \n",
    "\n",
    "This cell adds the functions to memory, plus does a bit of preprocessing. The functions themselves will be called once it's time to create the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sina.utils import ScalarRange\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# First, we figure out which record ids are associated with which dates\n",
    "records_at_dates = {}\n",
    "for date in dates:\n",
    "    records_at_dates[date] = set([str(x.object_id) for x in relationship_handler.get(subject_id=date,\n",
    "                                                                                     predicate=\"contains\")])\n",
    "    \n",
    "# Jupyter sometimes has an issue with the first call to plt.show(), so we make a dummy call\n",
    "plt.show()\n",
    "    \n",
    "def find_in_range(lat_start, long_start, lat_dec, long_inc):\n",
    "    \"\"\"Returns all Records in a coordinate square\"\"\"\n",
    "    latitude_req = ScalarRange(name=\"latitude\",\n",
    "                               min=lat_start - lat_dec,\n",
    "                               min_inclusive=True,\n",
    "                               max=lat_start,\n",
    "                               max_inclusive=False)\n",
    "    longitude_req = ScalarRange(name=\"longitude\",\n",
    "                                min=long_start,\n",
    "                                min_inclusive=True,\n",
    "                                max=long_start + long_inc,\n",
    "                                max_inclusive=False)\n",
    "    return record_handler.get_given_scalars((latitude_req,\n",
    "                                             longitude_req))\n",
    "    \n",
    "    \n",
    "def calculate_chunk(lat_start, long_start, lat_dec, long_inc):\n",
    "    \"\"\"\n",
    "    Calculate the avg_gcnorm and count for the chunk across each day in records_at_dates.\n",
    "    \n",
    "    Returns a dictionary mapping total and average gcnorm & num samples in a chunk to a day\n",
    "    \"\"\"\n",
    "    # print(\"Starting to find records\")\n",
    "    records = find_in_range(lat_start, long_start, lat_dec, long_inc)\n",
    "    # print(\"Finding scalars\")\n",
    "    out = defaultdict(lambda: {\"total\":0.0, \"count\":0, \"average\":0.0})\n",
    "    for record in records:\n",
    "        for date in records_at_dates:\n",
    "            if record.id in records_at_dates[date]:\n",
    "                out[date][\"total\"] += record.data[\"gcnorm\"][\"value\"]\n",
    "                out[date][\"count\"] += 1\n",
    "                break\n",
    "    for date in records_at_dates:\n",
    "        if out[date][\"count\"] > 0:\n",
    "            out[date][\"average\"] = out[date][\"total\"]/(out[date][\"count\"])\n",
    "    return(out)\n",
    "\n",
    "def calculate_chunk_fake(lat_start, long_start, lat_dec, long_inc):\n",
    "    \"\"\"\n",
    "    Calculate the avg_gcnorm and count for the chunk across each day in records_at_dates.\n",
    "    \n",
    "    Returns a dictionary mapping total and average gcnorm & num samples in a chunk to a day\n",
    "    \"\"\"\n",
    "    # print(\"Starting to find records\")\n",
    "    # records = find_in_range(lat_start, long_start, lat_dec, long_inc)\n",
    "    # print(lat_start, long_start, lat_inc, long_inc)\n",
    "    # print(\"Getting record IDs\")\n",
    "    # record_ids = set([x.id for x in records])\n",
    "    # print(\"Finding scalars\")\n",
    "    out = {}\n",
    "    for date in records_at_dates:\n",
    "        out[date] = {\"total\":120.0, \"count\":10, \"average\": random.randint(1,100)/1.0}\n",
    "    return(out)\n",
    "\n",
    "\n",
    "print(\"Functions loaded and date mappings built!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Graphs\n",
    "===============\n",
    "Now we divide up based on the number of chunks and collect the information for each chunk independently. Since we're only *reading* the underlying database, this could, in theory, be parallelized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "x_increment = (X_COORDS[1]-X_COORDS[0])/CHUNKS_PER_SIDE\n",
    "x_range = [x_increment*offset+X_COORDS[0] for offset in range(CHUNKS_PER_SIDE)]\n",
    "y_decrement = (Y_COORDS[1]-Y_COORDS[0])/CHUNKS_PER_SIDE\n",
    "y_range = list(reversed([y_decrement*offset+Y_COORDS[0] for offset in range(CHUNKS_PER_SIDE)]))\n",
    "\n",
    "def gen_plot():\n",
    "    avgs_at_date = defaultdict(lambda: np.zeros((CHUNKS_PER_SIDE, CHUNKS_PER_SIDE)))\n",
    "    counts_at_date = defaultdict(lambda: np.zeros((CHUNKS_PER_SIDE, CHUNKS_PER_SIDE)))\n",
    "    graphs_at_date = defaultdict(dict)       \n",
    "    chunks_completed = 0\n",
    "    for x_offset, x_coord in enumerate(x_range):\n",
    "        for y_offset, y_coord in enumerate(y_range):\n",
    "            norms_at_dates = calculate_chunk(lat_start = (y_coord + y_decrement), long_start = x_coord,\n",
    "                                             lat_dec = y_decrement, long_inc = x_increment)\n",
    "            chunks_completed += 1\n",
    "            progress = (\"Progress: {}/{}, finished ([{},{}), [{}, {}))\".format(chunks_completed,\n",
    "                                                                         CHUNKS_PER_SIDE**2,\n",
    "                                                                         x_coord,\n",
    "                                                                         x_coord + x_increment,\n",
    "                                                                         y_coord - y_decrement,\n",
    "                                                                         y_coord))\n",
    "            clear_output(wait=True)\n",
    "            display(progress)\n",
    "            for date in norms_at_dates:\n",
    "                # The y and x offsets are essentially coordinate positions here. So y is \"amount down\"\n",
    "                avgs_at_date[date][y_offset, x_offset] = (norms_at_dates[date].get(\"average\"))\n",
    "                counts_at_date[date][y_offset, x_offset] = (norms_at_dates[date][\"count\"])\n",
    "    print(\"Creating graph...\")\n",
    "    for date in dates:\n",
    "        fig, ax = plt.subplots(figsize=(7,7))\n",
    "        plt.xlabel('Longitude')\n",
    "        plt.ylabel('Latitude')\n",
    "        heatmap_avg = ax.imshow(avgs_at_date[date])\n",
    "        plt.colorbar(heatmap_avg, label=\"gcnorm\")\n",
    "        plt.title(\"Fukushima Radiation: Flight {}\".format(date))\n",
    "        scatter = ax.scatter(x=norm_x,\n",
    "                             y=norm_y,\n",
    "                             s=AREA_CITIES,\n",
    "                             c=COLOR_CITIES)\n",
    "        for x_coord, y_coord, city_info in zip(norm_x, norm_y, CITIES):\n",
    "            _, desc, alignment = city_info\n",
    "            ax.annotate(desc, (x_coord, y_coord), va=\"center\", ha=alignment, color=COLOR_CITIES)\n",
    "        # Matplotlib labels the boxes themselves, rather than the origin, so we need to calculate the centers\n",
    "        ax.set_xticks(range(len(x_range)))\n",
    "        ax.set_xticklabels((x+x_increment/2 for x in x_range))\n",
    "        ax.set_yticks(range(len(y_range)))\n",
    "        ax.set_yticklabels((y-y_decrement/2 for y in y_range))\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "                 rotation_mode=\"anchor\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "gen_plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sina",
   "language": "python",
   "name": "sina"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
